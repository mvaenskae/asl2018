\section{Throughput for Writes (90 pts)\label{sec:4}}

    \begin{table}
        \scriptsize{
            \begin{tabular}{|l|c|}
                \hline Number of servers                & 3 \\
                \hline Number of client machines        & 3 \\
                \hline Instances of memtier per machine & 2 \\
                \hline Threads per memtier instance     & 1 \\
                \hline Virtual clients per thread       & [1, 2, 4, 8, 16, 32, 48] \\
                \hline Workload                         & Write-Only \\
                \hline Multi-Get behavior               & N/A \\
                \hline Multi-Get size                   & N/A \\
                \hline Number of middlewares            & 2 \\
                \hline Worker threads per middleware    & [8, 16, 32, 64]  \\
                \hline
            \end{tabular}
            \caption{Experimental parameters for experiment 4.0.\label{tab:40_setup}}
        }
    \end{table}

    \subsection{Full System\label{subsec:4_full-system}}
        This experiment extends on experiment \ref{subsec:3_two-middlewares} and show how load-balancing SET requests
        amongst multiple \srv{}s affects performance. In contrast to that experiment 3 \srv{}s are used. The detailed
        configuration is listed in table \ref{tab:40_setup}.

        \begin{figure*}
            \vspace*{-.5\baselineskip}
            \makebox[1\linewidth][c]{%
                \centering
                \begin{subfigure}[t!]{0.55\textwidth}
                    \centering
                    \includegraphics[width=1\textwidth]{../data_analysis/figures/4-0_mw_throughput.png}
                    \caption{Request throughput.\label{fig:sets_mw_tp}}
                \end{subfigure}
                \begin{subfigure}[t!]{0.55\textwidth}
                    \centering
                    \includegraphics[width=1\textwidth]{../data_analysis/figures/4-0_mw_throughput-il.png}
                    \caption{Throughput calculation using the interactive law with an adjusted client
                             count.\label{fig:sets_mw_tp-il}}
                \end{subfigure}
            }
            \makebox[1\linewidth][c]{%
                \centering
                \begin{subfigure}[t!]{0.55\textwidth}
                    \centering
                    \includegraphics[width=1\textwidth]{../data_analysis/figures/4-0_mw_response_time.png}
                    \caption{Response times.\label{fig:sets_mw_rt}}
                \end{subfigure}
                \begin{subfigure}[t!]{0.55\textwidth}
                    \centering
                    \includegraphics[width=1\textwidth]{../data_analysis/figures/4-0_mw_queue-size.png}
                    \caption{Queue sizes.\label{fig:sets_mw_qs}}
                \end{subfigure}
            }
            \makebox[1\linewidth][c]{%
                \centering
                \begin{subfigure}[t!]{0.55\textwidth}
                    \centering
                    \includegraphics[width=1\textwidth]{../data_analysis/figures/4-0_mw_mc-comm-time.png}
                    \caption{Memcached communication time.\label{fig:sets_mw_mct}}
                \end{subfigure}
                \begin{subfigure}[t!]{0.55\textwidth}
                    \centering
                    \includegraphics[width=1\textwidth]{../data_analysis/figures/4-0_mw_queue-wait-time.png}
                    \caption{Queue waiting time.\label{fig:sets_mw_qwt}}
                \end{subfigure}
            }
            \caption{\mw{} statistics based on SET requests for various amounts of worker threads and active
                     clients.\label{fig:mw_sets_sll}}
        \end{figure*}

        \subsubsection{Explanation\label{subsubsec:4_full-system_explanation}}

            Compared to experiment \ref{subsec:3_two-middlewares} a decrease in performance can be observed for
            throughput. This follows from the fact that each \mw{} is connecting to three \srv{}s instead of just a
            single one and SETs need to be replicated amongst all connected \srv{}s. It is relevant to recall the design
            of the middleware. It may serve multiple requests at once but each request is handled in a single threaded
            fashion and as such no parallelization for communication between the middleware and memcached occurs.

            As in the previous experiment, throughput increases are observed for increasing amounts of worker threads
            and active clients in the system. In the case of 8 worker threads limited gains are observed for 24 clients,
            no gains after 48 clients; 16 worker threads have comparable behaviours for 48 and 96 clients; 32 worker
            threads for 96 and 192 clients and 64 worker threads for 192 and 288 clients. A clear trend can be observed
            thus with saturation points approaching when three times the amount of clients connect to the current worker
            thread configuration and actually reached for four times the amount of worker threads. This reflects in the
            queue sizes seen in figure \ref{fig:sets_mw_qs}. For the aforementioned amount of clients a change in
            steepness can be observed which shows the queue beginning to fill up stronger. As a consequence queue
            waiting times grow which means the overall response time of the system increases. Yet small throughput
            increases are expected as now each worker thread can immediately get an element from the queue without
            having to wait on said queue. This claim assumes the memcached communication time to also stay constant
            between the amounts of clients for limited and no gains. This holds well for configurations of 8\textendash
            32 worker threads. For 64 worker this is not strictly speaking observed but overall the gains of throughput
            are small enough to claim the system is well saturated as indicated by the growing queue.

            One note on the queue size on the previous claim to hold is that the queue must be larger than the amount of
            current worker threads. As such each worker thread has the guarantee to pick up a new task without having
            to wait for said task to be generated.

    \subsection{Summary\label{subsec:4_summary}}

        \begin{table}
            \def\sym#1{\ifmmode^{#1}\else\(^{#1}\)\fi}%
            \small{
                \centering
                \begin{tabular}{l*{8}{c}}
                    \toprule
                    & & \multicolumn{3}{c}{Throughput}  & \multicolumn{2}{c}{Average Time} & \\
                    \cmidrule(lr){3-5}\cmidrule(lr){6-7}
                    WT & Clients & \mw      & \mw{} \textendash{} IL & \cli     & Memcached & Queue & Queue Size & \\
                    \midrule
                    8  & 48      & 6680.07  & 6503.86                & 6675.16  & 2.28      & 3.81  & 12.72 \\
                    16 & 96      & 8616.11  & 8448.11                & 8611.22  & 3.57      & 6.27  & 27.00 \\
                    32 & 192     & 10327.79 & 10187.26               & 10323.51 & 6.01      & 10.69 & 55.18 \\
                    64 & 288     & 11469.24 & 11307.79               & 11463.20 & 10.84     & 10.73 & 61.54 \\
                    \addlinespace
                    \bottomrule
                \end{tabular}
                \caption{Evaluation of maximum throughputs of SETs for a full system. IL denotes the Interactive Law
                applied on the response time applying a thinking time based on the experiment.\label{tab:4_throughput-summary}}
            }
        \end{table}

        The numbers presented are derived applying the reasoning in subsubsection
        \ref{subsubsec:4_full-system_explanation}. It can be observed that the trend to an increase in throughput with
        the amount of worker threads is valid yet at a certain point diminishing returns are to be expected. The
        throughput deltas are (rounded to integers) 1936, 1712 and 1141 and illustrate the point of not scaling linearly
        with the amount of worker threads. Additionally the throughput measured by \mw{}s and \cli{}s match up well. The
        same argument is applicable to the interactive law. Numbers are in general a bit lower than measured but this
        is expected as numerical evaluations on the middleware are not able to generate infinitely-precise statistics.
        Verifying the interactive law (figure \ref{fig:sets_mw_tp-il}) shows very similar throughput graphs obtained for
        the middleware.

        % The derived throughput from the middleware can be explained as being different by the fact that the interactive law
        % applied assumed a thinking time of 0. This obviously is violated for the middleware as each request's response
        % time misses out on the communication time and packet handling between memtier and the middleware. A fixed offset
        % is not expected to exist as for each given throughput the amount of clients also grows, yet \cli{}s are
        % physically only 2 core machines. As such for increasing the number of clients more scheduling overhead is
        % expected. This means the interactive law, with no thinking time, applied on the response time derived from the
        % middleware is violated. Applying it on the throughput is valid though as a closed system is used. As such the
        % throughput and response time are expected to be correlated by taking the inverse to get the respectively other
        % metric. The results from the interactive law reported is a best effort approach using ping to infer network
        % latencies. The resulting numbers expect too little throughput using the interactive law which calculates the
        % number of active clients as listen in the summary of experiment \ref{sec:3}. This is an indication that either
        % the system response time reported is overestimated or that 
        %
        % \todo{Discuss this}.

        A note on the queue sizes. They are for all but the case of 64 worker threads above said amount. The system is
        therefore expected to actually be slightly oversaturated for all points whereas for 64 worker threads the point
        of perfect saturation is reached to within a couple percent.

        We can therefore derive that in general between three to four times the amount of clients for worker threads is
        the optimal setup for write only workloads if the minimum amount of threads is to be used. There is no
        measurable loss of performance in adding more worker threads.
