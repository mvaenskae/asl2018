\section{Gets and Multi-gets (90 pts)\label{sec:5}}

    In this section the system is analyzed for mixed workloads whereby SET requests and GET requests are in the ration
    1:1 but for GET the amount of keys requested is increased in addition to the middleware possibly splitting up GET
    requests with multiple keys fairly to each \srv{}s. The experiment aims to find out under what conditions this
    splitting, also called \emph{sharding} is preferable and how the throughput changes amongst differently sized GET
    requests. In table \ref{tab:50_setup} the experimental setup is documented.

    The experiment intends to use the most performant middleware as such the correct number of worker threads needs to
    be inferred for the case of 12 total clients. This experiment has two limiting factors:
    \begin{itemize}
        \item GET requests are network bound on the end of each \srv{}. The number of requests is expected to go down
              for more keys as replies get larger.
        \item SET requests are CPU bound on the end of each \cli{}. The number of requests is expected to not change
              due to CPU bounds but because the corresponding amount of GET requests is going down.
    \end{itemize}
    From experiment \ref{sec:4} it becomes clear that there is no significantly measurable performance impact using more
    worker threads than is required for clients. This means that any amount of worker threads beyond 8 is appropriate to
    use for this experiment for SETs. Experiment \ref{sec:3} is of minimal use as only single \srv{} configurations were
    tested. Yet it can be deduced that the middleware can handle at least one \srv{}s load of single-key GET replies
    without any problems. As no significantly measurable performance impact for SETs exist and multi-key GET requests
    are not yet observed with three \srv{}s active the decision was made to use 64 worker threads to exclude any
    performance impacts possible due to too little threading on the middleware.

    Important in this section's data is the exclusion of the \SI{60}{\second} stable-phase window as histograms are
    used. Memtier reports histograms for the complete run and not a configurable stable window. This decision was made
    to allow comparing not only data from the middleware and memtier but also relate it to histograms.

    \begin{table}
        \scriptsize{
            \begin{tabular}{|l|c|}
                \hline Number of servers                & 3 \\
                \hline Number of client machines        & 3 \\
                \hline Instances of memtier per machine & 2 \\
                \hline Threads per memtier instance     & 1 \\
                \hline Virtual clients per thread       & 2 \\
                \hline Workload                         & ratio=1:$<$Multi-Get size$>$ \\
                \hline Multi-Get behavior               & Sharded (5.1) and Non-Sharded (5.2) \\
                \hline Multi-Get size                   & [1, 3, 6, 9] \\
                \hline Number of middlewares            & 2 \\
                \hline Worker threads per middleware    & 64 \\
                \hline
            \end{tabular}
        }
            \caption{Experimental parameters for experiments 5.1 and 5.2.\label{tab:50_setup}}
    \end{table}

    \subsection{Sharded Case\label{subsec:5_sharded}}

        \begin{figure*}
            \vspace*{-.5\baselineskip}
                \centering
            \begin{subfigure}[t!]{0.43\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../data_analysis/figures/5-1_mt_response-time.png}
                \caption{Average response times.\label{fig:shard_mt_rt}}
            \end{subfigure}
            \begin{subfigure}[t!]{0.56\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../data_analysis/figures/5-1_mt_percentiles.png}
                \caption{Percentiles of all response times.\label{fig:shard_mt_rt_percentiles}}
            \end{subfigure}
            \caption{Response times measured on \cli{}s. The graphs include error bars and represent response times
            for GET packets using sharded behaviour on the \mw.\label{fig:shard_mt}}
        \end{figure*}

        \begin{figure*}
            \vspace*{-.5\baselineskip}
                \centering
            \begin{subfigure}[t!]{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../data_analysis/figures/5-1_mw_thoughput.png}
                \caption{Request throughput.\label{fig:shard_mw_tp}}
                \includegraphics[width=\textwidth]{../data_analysis/figures/5-1_mw_response-time.png}
                \caption{Response times.\label{fig:shard_mw_rt}}
            \end{subfigure}
            \begin{subfigure}[t!]{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../data_analysis/figures/5-1_mw_queue-size.png}
                \caption{Queue sizes.\label{fig:shard_mw_qs}}
                \includegraphics[width=\textwidth]{../data_analysis/figures/5-1_mw_mc-comm-time.png}
                \caption{Memcached communication time.\label{fig:shard_mw_mct}}
            \end{subfigure}
            \caption{\mw{} statistics based on GET requests. Requests are defined as consisting of one GET request
                     followed by a variable amount of keys to be requested.\label{fig:shard_mw}}
        \end{figure*}

        \subsubsection{Explanation\label{subsubsec:5_sharded_summary}}

            As can be seen in the subfigures of figure \ref{fig:shard_mt} the more keys are requested in a single GET
            request the slower the average response becomes. There is a rough linear behaviour in increasing response
            times for larger keys which is expected. Also it is of interest to note the stability of the response times;
            the system looks to be working optimally without outside interference.

            It is surprising to observe that the 50-percentile is not representative of the average response time. The
            75-percentile represents the value much closer. This signifies that there is a skew in the plot. This is
            also visible in the histograms obtained for 6-keys (see figure \ref{fig:histogram_sharded_mw}).
            % Even though differently sized GET requests are not easily comparable their percentiles grow in a similar
            % fashion. This shows their distributions are all based on the same base model though.

            For all requests the queue in the middleware is almost always empty. This shows the middleware performing
            well in terms of request handling. The hypothesis is therefore the middleware must be spending lots of time
            on the communication and packet handling with memcached. Figure \ref{fig:shard_mw_mct} visualizes the
            average communication and packet handling time per key size. Comparing the graph with figure
            \ref{fig:shard_mt_rt} the hypothesis is verified. A discussion comparing this behaviour with no sharding
            follows in subsection \ref{subsec:5_summary}.\newline
            The throughput reported by the middleware in figure \ref{fig:shard_mw_tp} shows a downwards trend for
            requests processed. This is expected as even though fewer requests are made, the throughput in keys
            requested increases. The resulting throughputs in keys are 2879, 6875, 8634 and 8786 keys respectively. The
            trend downwards is in general hitting a floor due to GET requests being network bound on the side of
            memcached (cf. table \ref{tab:iperf_results}).
            The response time matches quite well with the memcached response times. This indicates that the middleware
            is easily able to send the requests.

    \subsection{Non-sharded Case\label{subsec:5_nonsharded}}

        \begin{figure*}
            \vspace*{-.5\baselineskip}
                \centering
            \begin{subfigure}[t!]{0.43\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../data_analysis/figures/5-2_mt_response-time.png}
                \caption{Average response times.\label{fig:noshard_mt_rt}}
            \end{subfigure}
            \begin{subfigure}[t!]{0.56\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../data_analysis/figures/5-2_mt_percentiles.png}
                \caption{Percentiles of all response times.\label{fig:noshard_mt_rt_percentiles}}
            \end{subfigure}
            \caption{Response times measured on \cli{}s. The graphs include error bars and represent response times
                     for GET packets using non-sharded behaviour on the \mw.\label{fig:noshard_mt}}
        \end{figure*}

        \begin{figure*}
            \vspace*{-.5\baselineskip}
                \centering
            \begin{subfigure}[t!]{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../data_analysis/figures/5-2_mw_thoughput.png}
                \caption{Request throughput.\label{fig:noshard_mw_tp}}
                \includegraphics[width=\textwidth]{../data_analysis/figures/5-2_mw_response-time.png}
                \caption{Average response times.\label{fig:noshard_mw_rt}}
            \end{subfigure}
            \begin{subfigure}[t!]{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../data_analysis/figures/5-2_mw_queue-size.png}
                \caption{Queue sizes.\label{fig:noshard_mw_qs}}
                \includegraphics[width=\textwidth]{../data_analysis/figures/5-2_mw_mc-comm-time.png}
                \caption{Memcached communication time.\label{fig:noshard_mw_mct}}
            \end{subfigure}
            \caption{\mw{} statistics based on GET requests. Requests are defined as consisting of one GET request
                     followed by a variable amount of keys to be requested.\label{fig:noshard_mw}}
        \end{figure*}

        \subsubsection{Explanation\label{subsubsec:5_nonsharded_summary}}

            The subfigures of figure \ref{fig:noshard_mt} show similar behaviour to the ones in figure
            \ref{fig:shard_mt}. Important differences are the decreased 50-percentile point for mutliGETs of size 9
            with higher response times for 6 and 9 keys in the high percentiles. This indicates that sharding helps
            minimize GET request variance at the cost of a higher average response time. This argument is strengthened
            when comparing the average response times of figures \ref{fig:noshard_mt_rt} and \ref{fig:shard_mt_rt}. For
            non-sharding the average response times are therefore lower (about \SI{2}{\milli\second} for 9 keys and
            about \SI{1}{\milli\second} for 6-keys) but higher variances are to be expected. For both experiments the
            response time for keys of size 1 are virtually identical. It is noteworthy to state that the distribution
            for 3-keys and 1-keys basically overlap. This seems to show a perfect scaling from GETs with 1 key to 3 keys
            by the system in the case of not sharding.

            % The distribution of percentiles at the 50-percentile matches the average response time graph which is expected.
            % Even though differently sized GET requests are not easily comparable their percentiles grow in a similar
            % fashion. This shows their distributions are all based on the same base model.

            The queue is similarly to the sharded case nearly empty and the middleware spends most of the time
            communicating with a single \srv{} as opposed to multiple ones for GET requests (no sharding occurs). As
            such only a single connection is used which adds the benefit of being able to receive more contiguous data
            from a single connection, meaning more stable behaviour once a replies occurs. This behaviour will be
            compared in subsection \ref{subsec:5_summary}.\newline
            The reported throughput has a slightly stronger bent after 3 keys compared to sharded responses but overall
            the throughputs are comparable between sharded and non-sharded. A closer comparison shows the system
            performing better for GETs with 3 keys and not using sharding (2543 for non-sharded and 2292 for
            sharded). The respective keys exchanged are 2868, 7628, 8722, 8887. A higher throughput is achieved compared
            to the sharded case. Lastly the latency is included which indicates a linear increase in latency from 3
            \textendash{} 9 keys. This trend is reasonable insofar that the network bound is approached and data
            expected to take longer to send.
            Again the response time matches quite well with the memcached response times. This indicates that the
            middleware is easily able to send the requests.

    \subsection{Histogram\label{subsec:5_histograms}}

        \begin{figure*}
            \vspace*{-.5\baselineskip}
                \centering
            \begin{subfigure}[t!]{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../data_analysis/figures/5-1_mt_histogram.png}
                \caption{Histogram for sharded multiGET with 6 keys measured on \cli.\label{fig:histogram_sharded_mt}}
                \includegraphics[width=\textwidth]{../data_analysis/figures/5-1_mw_histogram.png}
                \caption{Histogram for sharded multiGET with 6 keys measured on \mw.\label{fig:histogram_sharded_mw}}
            \end{subfigure}
            \begin{subfigure}[t!]{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../data_analysis/figures/5-2_mt_histogram.png}
                \caption{Histogram for non-sharded mutliGET with 6 keys measured on
                \cli.\label{fig:histogram_nonsharded_mt}}
                \includegraphics[width=\textwidth]{../data_analysis/figures/5-2_mw_histogram.png}
                \caption{Histogram for non-sharded mutliGET with 6 keys measured on
                \mw.\label{fig:histogram_nonsharded_nw}}
            \end{subfigure}
            \caption{Histograms of response times measured for sharded and non-sharded multiGETs of 6 keys. The
                     distribution is based on GET requests and a bucket sizes of \SI{0.1}{\milli\second} is used, with the last
                     bucket accumulating all values $\geq$ \SI{19.9}{\milli\second}. The histogram lacks error bars as
                     the three repetitions' histogram results were extracted and seen as one graph. Thus no error bars
                     are able to be display.\label{fig:histograms}}
        \end{figure*}

        The histograms show consistent behaviour between \srv{}s and \mw{}s; the response times are never smaller for
        \srv{}s than for \mw{}s and the shape measured being smoother on the middleware than on memtier follows from
        a constant bucketing of \SI{0.1}{\milli\second} on the middleware as opposed to memtier which accumulates
        responses beyond \SI{10}{\milli\second} into buckets of \SI{1}{\milli\second}. As such they appear more coarse
        after \SI{10}{\milli\second}.

        The general shapes are similar between measurements at \mw{}s and \cli{}s for the case of sharded and
        non-sharded multi-key GET requests. The peaks are much more pronounced on the middleware which makes sense as it
        is closer to the clients and can process data quicker due to their multithreaded nature. The respective
        memcached plots share similar peaks; the system can be seen as equally under load in both cases.

        Noteworthy in figure \ref{fig:histogram_sharded_mw} is the presence of a small yet measureable second hill
        between \SIrange{7.5}{10.0}{\milli\second}. It is also visible on the respective \cli{} measurement but doesn't
        exist for the non-sharded case. This behaviour may be due to servers having a variable response time and with
        each sharded request convolving the expected response times from each server. This means there is a coupling of
        all servers and thus a perturbation towards the tail's end is expected. Due to sharded requests being more
        evenly distributed the tail must be shorter compared to the non-sharded case. The previous graphs on percentiles
        reflect well on the distributions displayed. Similar behaviour is observed for the non-sharded graph.

        Additionally the histograms show very few outliers in the last bucket. The represented curves are therefore well
        suited to explain the distribution of response times.

        Lastly the previous claim of sharding increasing the response time yet stabilizing the variance shows well in
        the plots. In general sharded results cut off earlier than the non-sharded counter-part as can be seen when
        looking at the tails of both experiments. In the non-sharded experiment a long thin tail is observed.

    \subsection{Summary\label{subsec:5_summary}}

        The question on how much sharding improves network performance is not easy to answer. Overall the throughput
        stays mostly comparable for GETs with same key sizes (exclusion being 3-keys).

        The only measurable metric that may be of interest is the reduced response time for the non-sharded case. But
        this only shows strongly for large keys. This seems to indicate that coordinating keys between three \srv{}s is
        more cumbersome than just sending the request to one. Looking at how connection handling and memcached
        communication is designed on the middleware it becomes clear that distributing the GET requests to the servers
        is done quickly. Accepting replies and reassembling the responses is the bottleneck. Even though many worker
        threads exist on the middleware each one handles network communication in a single-threaded approach. As such
        responses begin to be serialized if they arrive simultaneously on the middleware (the thread can only handle one
        connection at a time) and introduce the negative side-effect of requiring more state computations and OS-based
        calls to access data on each of the sockets used for communication. As requests are also guaranteed to be over
        the size of an MTU and the design of network communication returns once no new data is present in the socket bad
        interleavings of accessing partially complete data on sockets introduce further overhead. This overhead is not
        as strongly present for a single connection. Using a single connection introduces the issue of two requests
        being sent to the same \srv{}. As such a larger variation is expected as memcached is run single threaded.

        In short it is therefore when expecting averages to be low sensible to not use sharding but enabling it when
        more consistent response times are requested for increasing key sizes. For 3 keys in a GET the favourite is to
        not enable sharding.

        In both cases there is a measurable increase in throughput for GET as key increase in size. It approaches the
        upload limit of all three servers. As such the distribution of requests follows a good pattern where the system
        can show its true performance. It becomes clear that increasing the amount of keys per request is therefore one
        way to increase performance for GET requests.

        The interactive law cannot be simply applied to the throughput numbers obtained in figures \ref{fig:shard_mw_tp}
        and \ref{fig:noshard_mw_tp}. Calculating the true response time it becomes apparent that the numbers don't match up.
        This stems from the fact that only half the total requests are displayed and as discussed prior the thinking
        time is missing. The graph only display GET requests, not both, SET \& GET requests. As they are send in a 1:1
        ratio though, simply by multiplying the amount of requests for response time calculation gives a very good
        approximation of the average response time for GETs and SETs.
